{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "annoying-withdrawal",
   "metadata": {},
   "source": [
    "# tf.Dataset, batch, window, flat_map을 활용한 loader 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "earned-equipment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "daily_sales_numbers = [21, 22, -100, 31, -1, 32, 34, 31]\n",
    "\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices(daily_sales_numbers)\n",
    "tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aware-farmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(21, shape=(), dtype=int32)\n",
      "tf.Tensor(22, shape=(), dtype=int32)\n",
      "tf.Tensor(-100, shape=(), dtype=int32)\n",
      "tf.Tensor(31, shape=(), dtype=int32)\n",
      "tf.Tensor(-1, shape=(), dtype=int32)\n",
      "tf.Tensor(32, shape=(), dtype=int32)\n",
      "tf.Tensor(34, shape=(), dtype=int32)\n",
      "tf.Tensor(31, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for sales in tf_dataset:\n",
    "    print(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-project",
   "metadata": {},
   "source": [
    "### as_numpy_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "manual-calculator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "22\n",
      "-100\n",
      "31\n",
      "-1\n",
      "32\n",
      "34\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "for sales in tf_dataset.as_numpy_iterator():\n",
    "    print(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-alloy",
   "metadata": {},
   "source": [
    "### take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "together-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "22\n",
      "-100\n"
     ]
    }
   ],
   "source": [
    "for sales in tf_dataset.take(3):\n",
    "    print(sales.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-motion",
   "metadata": {},
   "source": [
    "### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conservative-roberts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "22\n",
      "31\n",
      "32\n",
      "34\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "tf_dataset = tf_dataset.filter(lambda x: x > 0)\n",
    "\n",
    "for sales in tf_dataset.as_numpy_iterator():\n",
    "    print(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-gardening",
   "metadata": {},
   "source": [
    "### map\n",
    "\n",
    "- Dataset 전체에 함수를 맵핑합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "boxed-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "44\n",
      "62\n",
      "64\n",
      "68\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "tf_dataset = tf_dataset.map(lambda x: x * 2)\n",
    "\n",
    "for sales in tf_dataset.as_numpy_iterator():\n",
    "    print(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-nirvana",
   "metadata": {},
   "source": [
    "### shuffle\n",
    "- 데이터세트는 buffer_size 요소로 버퍼를 채운 다음이 버퍼에서 요소를 무작위로 샘플링하여 선택한 요소를 새 요소로 바꿉니다.\n",
    "\n",
    "- 완벽한 셔플 링을 위해서는 데이터 세트의 전체 크기보다 크거나 같은 버퍼 크기가 필요합니다.\n",
    "\n",
    "- 예를 들어, 데이터 집합에 10,000 개의 element가 있지만 buffer_size가 1,000으로 설정된 경우 셔플은 처음에 버퍼의 처음 1,000 개 element 중 임의의 element 만 선택합니다.\n",
    "\n",
    "- element가 선택되면 버퍼의 공간이 다음 element (즉, 1,001-st)로 대체되어 1,000 element 버퍼를 유지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "julian-apollo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "62\n",
      "64\n",
      "68\n",
      "62\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "tf_dataset = tf_dataset.shuffle(10)\n",
    "for sales in tf_dataset.as_numpy_iterator():\n",
    "    print(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "expired-programming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68 42]\n",
      "[62 44]\n",
      "[64 62]\n"
     ]
    }
   ],
   "source": [
    "for sales_batch in tf_dataset.batch(2):\n",
    "    print(sales_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "liberal-proportion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68 42 62]\n",
      "[64 62 44]\n"
     ]
    }
   ],
   "source": [
    "for sales_batch in tf_dataset.batch(3):\n",
    "    print(sales_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "coordinated-encounter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62 42 44 68]\n",
      "[64 62]\n"
     ]
    }
   ],
   "source": [
    "for sales in tf_dataset.batch(4):\n",
    "    print(sales.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sitting-premium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62 68 44 64]\n"
     ]
    }
   ],
   "source": [
    "for sales in tf_dataset.batch(4, drop_remainder=True):\n",
    "    print(sales.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-mailman",
   "metadata": {},
   "source": [
    "- 위 모든 것을 한 줄로 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "prescription-commissioner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62 64 42 44]\n"
     ]
    }
   ],
   "source": [
    "daily_sales_numbers = [21, 22, -100, 31, -1, 32, 34, 31]\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices(daily_sales_numbers)\n",
    "tf_dataset = tf_dataset.filter(lambda x: x > 0) \\\n",
    "        .map(lambda y: y * 2).shuffle(3).batch(4, drop_remainder=True)\n",
    "\n",
    "for sales_batch in tf_dataset.as_numpy_iterator():\n",
    "    print(sales_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-andrew",
   "metadata": {},
   "source": [
    "## batch, repeat  간의 관계 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-refund",
   "metadata": {},
   "source": [
    "### How `ds.repeat()` works\n",
    "\n",
    "- 반복할 때마다 데이터셋을 다시 초기화\n",
    "- element의 무한 시퀀스를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "medieval-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3) (1, 4) (2, 2) (3, 5) (4, 1) (5, 6) (6, 2) (7, 1) (8, 3) (9, 4) (10, 5) (11, 6) (12, 3) (13, 2) (14, 4) (15, 1) (16, 5) (17, 6) (18, 2) (19, 4) (20, 5) "
     ]
    }
   ],
   "source": [
    "arr = [1, 2, 3, 4, 5, 6]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(arr)\n",
    "dataset = dataset.shuffle(buffer_size=3).repeat()\n",
    "\n",
    "for idx, elem in enumerate(dataset):\n",
    "  print((idx, elem.numpy()), end=\" \")\n",
    "  if idx >= 20:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-suicide",
   "metadata": {},
   "source": [
    "### What will `ds.repeat().batch()` produce  \n",
    "\n",
    "- 배치 전에 `ds.repeat()`가 있으므로 데이터 생성이 계속됩니다. 그러나 batch내 element의 순서는 `ds.random()`으로 인해 달라집니다. 고려해야 할 점은 랜덤 버퍼의 크기로 인해 6이 첫 번째 배치에 존재하지 않는다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "threaded-rolling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2 4 3]\n",
      "1 [1 5 6]\n",
      "2 [1 3 5]\n",
      "3 [2 4 6]\n",
      "4 [2 1 5]\n",
      "5 [4 3 6]\n"
     ]
    }
   ],
   "source": [
    "arr = [1, 2, 3, 4, 5, 6]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(arr)\n",
    "dataset = dataset.shuffle(buffer_size=3).repeat().batch(3)\n",
    "\n",
    "for idx, elem in enumerate(dataset):\n",
    "  print(idx, elem.numpy())\n",
    "  if idx >= 5:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-invasion",
   "metadata": {},
   "source": [
    "## window: Time Series 데이터셋 생성에 유용\n",
    "- window: 그룹화 할 윈도우 크기(갯수)  \n",
    "- drop_remainder: 남은 부분을 버릴지 살릴지 여부  \n",
    "- shift는 1 iteration당 몇 개씩 이동할 것인지  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "prompt-guest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[3, 4, 5, 6, 7]\n",
      "[4, 5, 6, 7, 8]\n",
      "[5, 6, 7, 8, 9]\n",
      "[6, 7, 8, 9]\n",
      "[7, 8, 9]\n",
      "[8, 9]\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10) \n",
    "ds = ds.window(5, shift=1, drop_remainder=False)\n",
    "for d in ds:\n",
    "    print(list(d.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-status",
   "metadata": {},
   "source": [
    "### drop_remainder=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "destroyed-vertical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[3, 4, 5, 6, 7]\n",
      "[4, 5, 6, 7, 8]\n",
      "[5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10) \n",
    "ds = ds.window(5, shift=1, drop_remainder=True)\n",
    "for d in ds:\n",
    "    print(list(d.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-scroll",
   "metadata": {},
   "source": [
    "### shift=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "empirical-fraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[2, 3, 4, 5, 6]\n",
      "[4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10) \n",
    "ds = ds.window(5, shift=2, drop_remainder=True)\n",
    "for d in ds:\n",
    "    print(list(d.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-disclosure",
   "metadata": {},
   "source": [
    "### `flat_map()`\n",
    "- flat_map은 dataset에 함수를 apply해주고, 결과를 flatten하게 펼쳐 줍니다.\n",
    "\n",
    "- 아래는 lambda 함수를 통해 5개의 batch를 읽어들인 뒤 flatten된 리턴값을 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "statewide-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "dataset = dataset.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "accomplished-salvation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2], shape=(3,), dtype=int64)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int64)\n",
      "tf.Tensor([2 3 4], shape=(3,), dtype=int64)\n",
      "tf.Tensor([3 4 5], shape=(3,), dtype=int64)\n",
      "tf.Tensor([4 5 6], shape=(3,), dtype=int64)\n",
      "tf.Tensor([5 6 7], shape=(3,), dtype=int64)\n",
      "tf.Tensor([6 7 8], shape=(3,), dtype=int64)\n",
      "tf.Tensor([7 8 9], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10) \n",
    "ds = ds.window(3, shift=1, drop_remainder=True)\n",
    "ds = ds.flat_map(lambda w: w.batch(3))\n",
    "for d in ds:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-crossing",
   "metadata": {},
   "source": [
    "### `map()`\n",
    "- Time Series Dataset을 만드려는 경우, train/label 값을 분류하는 용도로 `map()`을 활용할 수 있습니다.\n",
    "\n",
    "- `x[:-1]`, `x[-1:]` 의 의도는 각 row의 마지막 index 전까지는 train data로, 마지막 index는 label로 활용하겠다는 의도입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "desperate-horizontal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6] [7]\n",
      "[0 1 2 3 4] [5]\n",
      "[4 5 6 7 8] [9]\n",
      "[3 4 5 6 7] [8]\n",
      "[1 2 3 4 5] [6]\n"
     ]
    }
   ],
   "source": [
    "window_size=5\n",
    "\n",
    "ds = tf.data.Dataset.range(10) \n",
    "ds = ds.window(window_size+1, shift=1, drop_remainder=True)\n",
    "ds = ds.flat_map(lambda w: w.batch(window_size+1))\n",
    "ds = ds.shuffle(10)\n",
    "ds = ds.map(lambda x: (x[:-1], x[-1:])) # 첫 4개와 마지막 1개를 분리\n",
    "\n",
    "for x, y in ds:\n",
    "    print(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-crossing",
   "metadata": {},
   "source": [
    "### 실습 예제: Sunspots 데이터셋을 활용하여 window_dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "artistic-emission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sunspots.csv', <http.client.HTTPMessage at 0x7fddc5c2a390>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib\n",
    "\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
    "urllib.request.urlretrieve(url, 'sunspots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sized-culture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1749-01-31', '96.7']\n",
      "['1', '1749-02-28', '104.3']\n",
      "['2', '1749-03-31', '116.7']\n",
      "['3', '1749-04-30', '92.8']\n",
      "['4', '1749-05-31', '141.7']\n",
      "['5', '1749-06-30', '139.2']\n"
     ]
    }
   ],
   "source": [
    "with open('sunspots.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader) # 첫 줄은 header이므로 skip \n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        print(row)\n",
    "        i+=1\n",
    "        if i > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-employment",
   "metadata": {},
   "source": [
    "각 row의 2번 index의 데이터를 우리가 time series 데이터로 만들어 보려고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "internal-washer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96.7, 104.3, 116.7, 92.8, 141.7]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "with open('sunspots.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)  # 첫 줄은 header이므로 skip \n",
    "    for row in reader:\n",
    "        train_data.append(float(row[2]))\n",
    "        \n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "retired-costs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3235, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.asarray(train_data)\n",
    "train_data = np.expand_dims(train_data, 1)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-platform",
   "metadata": {},
   "source": [
    "과거의 3일의 데이터를 보고 4일 째의 데이터를 예측해야한다라고 가정한다면,  \n",
    "window_size = 3 + 1 로 잡아줍니다.  \n",
    "3개는 train data의 갯수, 1은 label의 갯수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "continent-century",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, None, 1, 1), (None, None, 1, 1)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 3\n",
    "batch_size = 3\n",
    "shuffle_buffer = 1000\n",
    "\n",
    "def windowed_dataset(data, window_size, batch_size, shuffle_buffer):\n",
    "    data = np.expand_dims(data, axis=1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    ds = ds.window(window_size+1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size+1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[-1:]))\n",
    "    return ds.batch(batch_size)\n",
    "\n",
    "ds = windowed_dataset(train_data, window_size, batch_size, shuffle_buffer)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "expressed-tours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(3, 3, 1, 1), dtype=float64, numpy=\n",
      "array([[[[ 73. ]],\n",
      "\n",
      "        [[ 85.5]],\n",
      "\n",
      "        [[ 47.5]]],\n",
      "\n",
      "\n",
      "       [[[  0. ]],\n",
      "\n",
      "        [[  0. ]],\n",
      "\n",
      "        [[  0. ]]],\n",
      "\n",
      "\n",
      "       [[[112.3]],\n",
      "\n",
      "        [[ 85.7]],\n",
      "\n",
      "        [[ 35.7]]]])>, <tf.Tensor: shape=(3, 1, 1, 1), dtype=float64, numpy=\n",
      "array([[[[29.2]]],\n",
      "\n",
      "\n",
      "       [[[ 0. ]]],\n",
      "\n",
      "\n",
      "       [[[66.5]]]])>)\n"
     ]
    }
   ],
   "source": [
    "for d in ds:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-montreal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
